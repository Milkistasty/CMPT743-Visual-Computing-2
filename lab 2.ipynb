{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Object Detection with YOLO (You only look once)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "#@title Import libraries\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.ops import batched_nms\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import pandas as pd\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Utils\n",
    "\n",
    "\n",
    "def convert_cellboxes(boxes):\n",
    "    #boxes: torch tensor (B, S, S, 30)\n",
    "    bboxes1 = boxes[..., 21:25]\n",
    "    bboxes2 = boxes[..., 26:30]\n",
    "    scores = torch.cat((boxes[..., 20].unsqueeze(0), boxes[..., 25].unsqueeze(0)), dim = 0)\n",
    "    best_box = scores.argmax(0).unsqueeze(-1)\n",
    "    best_boxes = bboxes1 * (1 - best_box) + best_box * bboxes2\n",
    "    x_cell, y_cell = best_boxes[..., 0:1], best_boxes[..., 1:2]\n",
    "    width_cell, height_cell = best_boxes[..., 2:3], best_boxes[..., 3:4]\n",
    "\n",
    "    cell_indices = torch.arange(7).repeat(7, 1).unsqueeze(-1).to(boxes.device)\n",
    "\n",
    "    x = (x_cell + cell_indices) / 7\n",
    "    y = (y_cell + cell_indices.permute(1, 0, 2)) / 7\n",
    "\n",
    "    width = width_cell / 7\n",
    "    height = height_cell / 7\n",
    "\n",
    "    converted_boxes = torch.cat((x, y, width, height), dim = -1)\n",
    "    predicted_class = boxes[..., :20].argmax(-1).unsqueeze(-1)\n",
    "    best_confidence = torch.max(boxes[..., 20], boxes[..., 25]).unsqueeze(-1)\n",
    "\n",
    "    converted_pred = torch.cat((predicted_class, best_confidence, converted_boxes), dim = -1)\n",
    "\n",
    "    return converted_pred\n",
    "\n",
    "def cellboxes_to_boxes(boxes, S = 7):\n",
    "    #boxes: torch tensor (B, S, S, 30)\n",
    "    converted_pred = convert_cellboxes(boxes)\n",
    "\n",
    "    B = converted_pred.size(0)\n",
    "\n",
    "    all_bboxes = converted_pred.reshape(B, S * S, -1)\n",
    "\n",
    "    return all_bboxes\n",
    "\n",
    "def non_max_suppression(bboxes, iou_threshold = 0.5, confidence_threshold = 0.4):\n",
    "    #bboxes: (N, 6)\n",
    "    #boxes: (N, 6) -> (N2, 4)\n",
    "\n",
    "    #prune boxes that have confidence < threshold\n",
    "    bboxes = bboxes[bboxes[:, 1] > confidence_threshold]\n",
    "    bboxes_coords = bboxes[:, 2:6]\n",
    "    scores = bboxes[:, 1]\n",
    "    class_bboxes = bboxes[:, 0]\n",
    "\n",
    "    # convert xywh to x1y1x2y2\n",
    "    x1 = bboxes_coords[:, 0] - bboxes_coords[:, 2] / 2\n",
    "    y1 = bboxes_coords[:, 1] - bboxes_coords[:, 3] / 2\n",
    "    x2 = bboxes_coords[:, 0] + bboxes_coords[:, 2] / 2\n",
    "    y2 = bboxes_coords[:, 1] + bboxes_coords[:, 3] / 2\n",
    "\n",
    "    bboxes_coords = torch.stack((x1, y1, x2, y2)).T\n",
    "\n",
    "    keep = batched_nms(bboxes_coords, scores, class_bboxes, iou_threshold=iou_threshold)\n",
    "\n",
    "    return bboxes[keep]\n",
    "\n",
    "def visualize_images(images, row_size):\n",
    "    #images: list of PIL images\n",
    "    #row_size: number of images per row\n",
    "    #returns: PIL image\n",
    "\n",
    "    num_images = len(images)\n",
    "    col_size = num_images // row_size\n",
    "    if num_images % row_size != 0:\n",
    "        col_size += 1\n",
    "\n",
    "    image_size = images[0].size[0]\n",
    "    canvas = Image.new('RGB', (image_size * row_size, image_size * col_size))\n",
    "    for i, image in enumerate(images):\n",
    "        canvas.paste(image, (image_size * (i % row_size), image_size * (i // row_size)))\n",
    "    return canvas\n",
    "\n",
    "def draw_boxes(image, gt_boxes, pred_boxes):\n",
    "    image = ((image.permute(1, 2, 0).cpu().detach().numpy() + 1) * 127.5).astype('uint8')\n",
    "    image_pred = image.copy()\n",
    "    image_gt = image.copy()\n",
    "    for box in gt_boxes:\n",
    "        x, y, w, h = box[3:]\n",
    "        x1 = int((x - w / 2) * 448)\n",
    "        y1 = int((y - h / 2) * 448)\n",
    "        x2 = int((x + w / 2) * 448)\n",
    "        y2 = int((y + h / 2) * 448)\n",
    "\n",
    "        image_gt = cv2.rectangle(image_gt, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "\n",
    "    for box in pred_boxes:\n",
    "        x, y, w, h = box[3:]\n",
    "        x1 = int((x - w / 2) * 448)\n",
    "        y1 = int((y - h / 2) * 448)\n",
    "        x2 = int((x + w / 2) * 448)\n",
    "        y2 = int((y + h / 2) * 448)\n",
    "\n",
    "        image_pred = cv2.rectangle(image_pred, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "\n",
    "    image_gt = Image.fromarray(image_gt)\n",
    "    image_pred = Image.fromarray(image_pred)\n",
    "\n",
    "    image = visualize_images([image_gt, image_pred], 2)\n",
    "\n",
    "    return image\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO: Intersection over Union (15 minutes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title TODO: Intersection over Union (15 minutes)\n",
    "\n",
    "def intersection_over_union(boxes_preds, boxes_labels, box_format = 'midpoint'):\n",
    "    # boxes_preds: (..., 4)\n",
    "    # boxes_labels: (..., 4)\n",
    "    #box format: format of input boxes\n",
    "    if box_format == 'midpoint':\n",
    "        box1_x1 = boxes_preds[..., 0:1] - boxes_preds[..., 2:3] / 2\n",
    "        box1_y1 = boxes_preds[..., 1:2] - boxes_preds[..., 3:4] / 2\n",
    "        box1_x2 = boxes_preds[..., 0:1] + boxes_preds[..., 2:3] / 2\n",
    "        box1_y2 = boxes_preds[..., 1:2] + boxes_preds[..., 3:4] / 2\n",
    "        box2_x1 = boxes_labels[..., 0:1] - boxes_labels[..., 2:3] / 2\n",
    "        box2_y1 = boxes_labels[..., 1:2] - boxes_labels[..., 3:4] / 2\n",
    "        box2_x2 = boxes_labels[..., 0:1] + boxes_labels[..., 2:3] / 2\n",
    "        box2_y2 = boxes_labels[..., 1:2] + boxes_labels[..., 3:4] / 2\n",
    "    elif box_format == 'corners':\n",
    "        box1_x1 = boxes_preds[..., 0:1]\n",
    "        box1_y1 = boxes_preds[..., 1:2]\n",
    "        box1_x2 = boxes_preds[..., 2:3]\n",
    "        box1_y2 = boxes_preds[..., 3:4]\n",
    "        box2_x1 = boxes_labels[..., 0:1]\n",
    "        box2_y1 = boxes_labels[..., 1:2]\n",
    "        box2_x2 = boxes_labels[..., 2:3]\n",
    "        box2_y2 = boxes_labels[..., 3:4]\n",
    "    else:\n",
    "        raise ValueError('Unknown box format.')\n",
    "\n",
    "    #TODO: find (x1, y1) and (x2, y2) of\n",
    "    x1 = torch.max(box1_x1, box2_x1)\n",
    "    y1 = torch.max(box1_y1, box2_y1)\n",
    "    x2 = torch.min(box1_x2, box2_x2)\n",
    "    y2 = torch.min(box1_y2, box2_y2)\n",
    "\n",
    "    #TODO: find intersection area\n",
    "    intersection = (x2 - x1).clamp(0) * (y2 - y1).clamp(0)\n",
    "    box1_area = torch.abs((box1_x2 - box1_x1) * (box1_y2 - box1_y1))\n",
    "    box2_area = torch.abs((box2_x2 - box2_x1) * (box2_y2 - box2_y1))\n",
    "    #TODO: find union area\n",
    "    union =  box1_area + box2_area - intersection\n",
    "\n",
    "    return intersection / (union + 1e-6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check IoU implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Check IoU implementation\n",
    "\n",
    "boxes1 = torch.tensor([[0.5, 0.5, 0.6, 0.3], [0.2, 0.2, 0.1, 0.1]])\n",
    "boxes2 = torch.tensor([[0.5, 0.5, 0.3, 0.6], [0.6, 0.6, 0.2, 0.3]])\n",
    "assert (torch.abs(intersection_over_union(boxes1,  boxes2).reshape(2, ) - torch.tensor([0.3333, 0.])) < 1e-4).all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download and config dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: command not found: gdown\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'small_data.zip'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/wenhe/Downloads/lab2.ipynb Cell 12\u001b[0m line \u001b[0;36m7\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/wenhe/Downloads/lab2.ipynb#X25sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mzipfile\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/wenhe/Downloads/lab2.ipynb#X25sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m get_ipython()\u001b[39m.\u001b[39msystem(\u001b[39m'\u001b[39m\u001b[39mgdown --id 1HIPUz1aguNp8RTSx_H-f7z9_OHOu35n5\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/wenhe/Downloads/lab2.ipynb#X25sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m zip_file \u001b[39m=\u001b[39m zipfile\u001b[39m.\u001b[39;49mZipFile(\u001b[39m'\u001b[39;49m\u001b[39msmall_data.zip\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/wenhe/Downloads/lab2.ipynb#X25sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m zip_file\u001b[39m.\u001b[39mextractall()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/wenhe/Downloads/lab2.ipynb#X25sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m zip_file\u001b[39m.\u001b[39mclose()\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.11/3.11.6_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/zipfile.py:1284\u001b[0m, in \u001b[0;36mZipFile.__init__\u001b[0;34m(self, file, mode, compression, allowZip64, compresslevel, strict_timestamps, metadata_encoding)\u001b[0m\n\u001b[1;32m   1282\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m   1283\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1284\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfp \u001b[39m=\u001b[39m io\u001b[39m.\u001b[39mopen(file, filemode)\n\u001b[1;32m   1285\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mOSError\u001b[39;00m:\n\u001b[1;32m   1286\u001b[0m         \u001b[39mif\u001b[39;00m filemode \u001b[39min\u001b[39;00m modeDict:\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'small_data.zip'"
     ]
    }
   ],
   "source": [
    "#@title Download dataset\n",
    "\n",
    "import zipfile\n",
    "\n",
    "!gdown --id 1HIPUz1aguNp8RTSx_H-f7z9_OHOu35n5\n",
    "\n",
    "zip_file = zipfile.ZipFile('small_data.zip')\n",
    "zip_file.extractall()\n",
    "\n",
    "zip_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Dataset class\n",
    "\n",
    "\n",
    "# label structure\n",
    "# 7*7*30\n",
    "# 7*7 -> overall num of cells\n",
    "# 30:\n",
    "# 0:19 -> box confidence (i.e. 20 classes, 1 for GT, 0 for others)\n",
    "# 20 -> box idx (1 for box, 0 for not box)\n",
    "# 21:24 -> box coordinates -> (x_cell, y_cell, width, height)\n",
    "# 25 -> object idx (1 for object, 0 for not object)\n",
    "# 26:29 -> object coordinates -> (x_cell, y_cell, width, height)\n",
    "\n",
    "class VOCDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data_dir, split = 'train', S=7, B=2, C=20, transform=None):\n",
    "        csv_file = os.path.join(data_dir, '100examples.csv')\n",
    "        self.annotations = pd.read_csv(csv_file)\n",
    "        self.image_dir = os.path.join(data_dir, 'images')\n",
    "        self.label_dir = os.path.join(data_dir, 'labels')\n",
    "        self.transform = transform\n",
    "        self.S = S\n",
    "        self.B = B\n",
    "        self.C = C\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.annotations)\n",
    "    def __getitem__(self, idx):\n",
    "        label_path = os.path.join(self.label_dir, self.annotations.iloc[idx, 1])\n",
    "        boxes = []\n",
    "        with open(label_path) as f:\n",
    "            for label in f.readlines():\n",
    "                class_label, x, y, width, height = [\n",
    "                    float(x) if float(x) != int(float(x)) else int(x)\n",
    "                    for x in label.replace('\\n', '').split()\n",
    "                ]\n",
    "                boxes.append([class_label, x, y, width, height])\n",
    "        img_path = os.path.join(self.image_dir, self.annotations.iloc[idx, 0])\n",
    "        image = Image.open(img_path)\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        label_matrix = torch.zeros((self.S, self.S, self.C + 5 * self.B))\n",
    "        for box in boxes:\n",
    "            class_label, x, y, width, height = box\n",
    "            class_label = int(class_label)\n",
    "            i, j = int(self.S * y), int(self.S * x)\n",
    "            x_cell, y_cell = self.S * x - j, self.S * y - i\n",
    "            width_cell, height_cell = (\n",
    "                width * self.S,\n",
    "                height * self.S\n",
    "            )\n",
    "            if label_matrix[i, j, 20] == 0:\n",
    "                label_matrix[i, j, 20] = 1\n",
    "                box_coordinates = torch.tensor(\n",
    "                    [x_cell, y_cell, width_cell, height_cell]\n",
    "                )\n",
    "                label_matrix[i, j, 21:25] = box_coordinates\n",
    "                label_matrix[i, j, class_label] = 1\n",
    "        return image, label_matrix\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO: Initialize dataset (5 minutes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './small_data/100examples.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/wenhe/Downloads/lab2.ipynb Cell 16\u001b[0m line \u001b[0;36m1\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/wenhe/Downloads/lab2.ipynb#X23sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m data_dir \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m./small_data\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/wenhe/Downloads/lab2.ipynb#X23sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m transform \u001b[39m=\u001b[39m transforms\u001b[39m.\u001b[39mCompose([\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/wenhe/Downloads/lab2.ipynb#X23sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m     transforms\u001b[39m.\u001b[39mResize((\u001b[39m448\u001b[39m, \u001b[39m448\u001b[39m)),  \u001b[39m# Resize the image to 448x448\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/wenhe/Downloads/lab2.ipynb#X23sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m     transforms\u001b[39m.\u001b[39mToTensor(),          \u001b[39m# Convert the image to a torch tensor\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/wenhe/Downloads/lab2.ipynb#X23sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m     transforms\u001b[39m.\u001b[39mNormalize(mean\u001b[39m=\u001b[39m[\u001b[39m0.5\u001b[39m, \u001b[39m0.5\u001b[39m, \u001b[39m0.5\u001b[39m], std\u001b[39m=\u001b[39m[\u001b[39m0.5\u001b[39m, \u001b[39m0.5\u001b[39m, \u001b[39m0.5\u001b[39m])  \u001b[39m# Normalize to (-1, 1)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/wenhe/Downloads/lab2.ipynb#X23sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m ])\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/wenhe/Downloads/lab2.ipynb#X23sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m train_dataset \u001b[39m=\u001b[39m VOCDataset(data_dir \u001b[39m=\u001b[39;49m data_dir, transform \u001b[39m=\u001b[39;49m transform)\n",
      "\u001b[1;32m/Users/wenhe/Downloads/lab2.ipynb Cell 16\u001b[0m line \u001b[0;36m6\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/wenhe/Downloads/lab2.ipynb#X23sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, data_dir, split \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m'\u001b[39m, S\u001b[39m=\u001b[39m\u001b[39m7\u001b[39m, B\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m, C\u001b[39m=\u001b[39m\u001b[39m20\u001b[39m, transform\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/wenhe/Downloads/lab2.ipynb#X23sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     csv_file \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(data_dir, \u001b[39m'\u001b[39m\u001b[39m100examples.csv\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/wenhe/Downloads/lab2.ipynb#X23sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mannotations \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39;49mread_csv(csv_file)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/wenhe/Downloads/lab2.ipynb#X23sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mimage_dir \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(data_dir, \u001b[39m'\u001b[39m\u001b[39mimages\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/wenhe/Downloads/lab2.ipynb#X23sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlabel_dir \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(data_dir, \u001b[39m'\u001b[39m\u001b[39mlabels\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/pandas/io/parsers/readers.py:948\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m    935\u001b[0m kwds_defaults \u001b[39m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    936\u001b[0m     dialect,\n\u001b[1;32m    937\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    944\u001b[0m     dtype_backend\u001b[39m=\u001b[39mdtype_backend,\n\u001b[1;32m    945\u001b[0m )\n\u001b[1;32m    946\u001b[0m kwds\u001b[39m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 948\u001b[0m \u001b[39mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/pandas/io/parsers/readers.py:611\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    608\u001b[0m _validate_names(kwds\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mnames\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m))\n\u001b[1;32m    610\u001b[0m \u001b[39m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 611\u001b[0m parser \u001b[39m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[1;32m    613\u001b[0m \u001b[39mif\u001b[39;00m chunksize \u001b[39mor\u001b[39;00m iterator:\n\u001b[1;32m    614\u001b[0m     \u001b[39mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1448\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1445\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptions[\u001b[39m\"\u001b[39m\u001b[39mhas_index_names\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m kwds[\u001b[39m\"\u001b[39m\u001b[39mhas_index_names\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m   1447\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles: IOHandles \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m-> 1448\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_engine \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_engine(f, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mengine)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1705\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1703\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m mode:\n\u001b[1;32m   1704\u001b[0m         mode \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m-> 1705\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles \u001b[39m=\u001b[39m get_handle(\n\u001b[1;32m   1706\u001b[0m     f,\n\u001b[1;32m   1707\u001b[0m     mode,\n\u001b[1;32m   1708\u001b[0m     encoding\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mencoding\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[1;32m   1709\u001b[0m     compression\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mcompression\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[1;32m   1710\u001b[0m     memory_map\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mmemory_map\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mFalse\u001b[39;49;00m),\n\u001b[1;32m   1711\u001b[0m     is_text\u001b[39m=\u001b[39;49mis_text,\n\u001b[1;32m   1712\u001b[0m     errors\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mencoding_errors\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mstrict\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[1;32m   1713\u001b[0m     storage_options\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mstorage_options\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[1;32m   1714\u001b[0m )\n\u001b[1;32m   1715\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   1716\u001b[0m f \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles\u001b[39m.\u001b[39mhandle\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/pandas/io/common.py:863\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    858\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(handle, \u001b[39mstr\u001b[39m):\n\u001b[1;32m    859\u001b[0m     \u001b[39m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    860\u001b[0m     \u001b[39m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    861\u001b[0m     \u001b[39mif\u001b[39;00m ioargs\u001b[39m.\u001b[39mencoding \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m ioargs\u001b[39m.\u001b[39mmode:\n\u001b[1;32m    862\u001b[0m         \u001b[39m# Encoding\u001b[39;00m\n\u001b[0;32m--> 863\u001b[0m         handle \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39;49m(\n\u001b[1;32m    864\u001b[0m             handle,\n\u001b[1;32m    865\u001b[0m             ioargs\u001b[39m.\u001b[39;49mmode,\n\u001b[1;32m    866\u001b[0m             encoding\u001b[39m=\u001b[39;49mioargs\u001b[39m.\u001b[39;49mencoding,\n\u001b[1;32m    867\u001b[0m             errors\u001b[39m=\u001b[39;49merrors,\n\u001b[1;32m    868\u001b[0m             newline\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    869\u001b[0m         )\n\u001b[1;32m    870\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    871\u001b[0m         \u001b[39m# Binary mode\u001b[39;00m\n\u001b[1;32m    872\u001b[0m         handle \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39m(handle, ioargs\u001b[39m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './small_data/100examples.csv'"
     ]
    }
   ],
   "source": [
    "#@title **TODO**: Initialize dataset (5 minutes)\n",
    "# TODO: Initialize the dataset with the correct transformations\n",
    "\n",
    "# Transformation should:\n",
    "# 1. resize the image to (448, 448)\n",
    "# 2. convert images to torch tensors with values (-1, 1)\n",
    "# Use transform.Compose([])\n",
    "\n",
    "data_dir = './small_data'\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((448, 448)),  # Resize the image to 448x448\n",
    "    transforms.ToTensor(),          # Convert the image to a torch tensor\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], \n",
    "                         std=[0.5, 0.5, 0.5])  # Normalize to (-1, 1)\n",
    "])\n",
    "\n",
    "train_dataset = VOCDataset(data_dir = data_dir, transform = transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO: Visualize images with bounding boxes (20 minutes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/wenhe/Downloads/lab2.ipynb Cell 18\u001b[0m line \u001b[0;36m4\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/wenhe/Downloads/lab2.ipynb#X40sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m#@title **TODO**: Visualize images with bounding boxes (20 minutes)\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/wenhe/Downloads/lab2.ipynb#X40sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/wenhe/Downloads/lab2.ipynb#X40sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39m# TODO: get an item from the dataset\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/wenhe/Downloads/lab2.ipynb#X40sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m sample_image, sample_label \u001b[39m=\u001b[39m train_dataset[\u001b[39m0\u001b[39m]\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/wenhe/Downloads/lab2.ipynb#X40sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39m# TODO: convert image to numpy array with shape (448, 448 , 3) in the range (0, 255) and type uint8\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/wenhe/Downloads/lab2.ipynb#X40sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m image \u001b[39m=\u001b[39m sample_image\u001b[39m.\u001b[39mpermute(\u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m, \u001b[39m0\u001b[39m)\u001b[39m.\u001b[39mnumpy()  \u001b[39m# Change channel order\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_dataset' is not defined"
     ]
    }
   ],
   "source": [
    "#@title **TODO**: Visualize images with bounding boxes (20 minutes)\n",
    "\n",
    "# TODO: get an item from the dataset\n",
    "sample_image, sample_label = train_dataset[0]\n",
    "\n",
    "# TODO: convert image to numpy array with shape (448, 448 , 3) in the range (0, 255) and type uint8\n",
    "image = sample_image.permute(1, 2, 0).numpy()  # Change channel order\n",
    "image = ((image * 0.5 + 0.5) * 255).astype(np.uint8)  # Denormalize and scale to (0, 255)\n",
    "\n",
    "H, W = image.shape[:2]\n",
    "# TODO: 1. find the cells in the image grid where there is a bounding box\n",
    "# 2. get the coordinates of the boxes\n",
    "# Hint: Use torch.where\n",
    "\n",
    "box_idx = torch.where(sample_label[:, :, 20] > 0)\n",
    "boxes = sample_label[box_idx[0], box_idx[1], 21:25]\n",
    "\n",
    "# print(box_idx, boxes)\n",
    "\n",
    "#TODO: iterate over the boxes and Compute the global coordinates of the boxes\n",
    "image = image.copy()\n",
    "for i in range(len(boxes)):\n",
    "  x_cell, y_cell = boxes[i, 0], boxes[i, 1]  # cell coordinate\n",
    "  width_cell, height_cell = boxes[i, 2], boxes[i, 3]  # cell coordinate\n",
    "\n",
    "  x = 448 * (x_cell + box_idx[1][i]) / 7\n",
    "  y = 448 * (y_cell + box_idx[0][i]) / 7\n",
    "  width = 448 * width_cell / 7\n",
    "  height = 448 * height_cell / 7\n",
    "  image = cv2.rectangle(image, (int(x - width / 2), int(y - height / 2)),\n",
    "                  (int(x + width / 2), int(y + height / 2)), (0, 0, 255), 2)\n",
    "  \n",
    "\n",
    "Image.fromarray(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Config Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO: CNN Block (10 minutes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title **TODO**: CNN Block (10 minutes)\n",
    "\n",
    "# A CNN Block consists of:\n",
    "## 1. A convolution layer\n",
    "## 2. A batch normalization layer\n",
    "## 3. A activation function. Use LeakyReLU with slop 0.1\n",
    "class CNNBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, **kwargs):\n",
    "        super(CNNBlock, self).__init__()\n",
    "        # Create a CNN block with Conv2d, BatchNorm2d, and LeakyReLU\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, **kwargs),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.LeakyReLU(0.1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO: YOLO Network (30 minutes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title **TODO**: YOLO Network (30 minutes)\n",
    "\n",
    "\n",
    "# architecure of the YOLO network\n",
    "# Tuple :(kernel size, out channels, stirde, padding)\n",
    "# 'M': 2x2 max pooling\n",
    "# List: contains Tuples with the same convention mentioned. The last element is number of repetitions\n",
    "\n",
    "architecture_config = [\n",
    "    (7, 64, 2, 3),\n",
    "    'M',\n",
    "    (3, 192, 1, 1),\n",
    "    'M',\n",
    "    (1, 128, 1, 0),\n",
    "    (3, 256, 1, 1),\n",
    "    (1, 256, 1, 0),\n",
    "    (3, 512, 1, 1),\n",
    "    'M',\n",
    "    [(1, 256, 1, 0), (3, 512, 1, 1), 4],\n",
    "    (1, 512, 1, 0),\n",
    "    (3, 1024, 1, 1),\n",
    "    'M',\n",
    "    [(1, 512, 1, 0), (3, 1024, 1, 1), 2],\n",
    "    (3, 1024, 1, 1),\n",
    "    (3, 1024, 2, 1),\n",
    "    (3, 1024, 1, 1),\n",
    "    (3, 1024, 1, 1),\n",
    "]\n",
    "\n",
    "class YOLONet(nn.Module):\n",
    "    def __init__(self, in_channel = 3, **kwargs):\n",
    "        super(YOLONet, self).__init__()\n",
    "        self.in_channel = in_channel\n",
    "        self.darknet = self._create_conv_layers()\n",
    "        self.fcs = self._create_fcs(**kwargs)\n",
    "\n",
    "    def forward(self, x):\n",
    "        #TODO 1. Pass to darknet 2. Flatten 3. Pass to FC layer 4. Returns\n",
    "        x = self.darknet(x)\n",
    "        x = torch.flatten(x, start_dim=1)  # Flatten the output for the fully connected layer\n",
    "        x = self.fcs(x)\n",
    "        return x\n",
    "\n",
    "    def _create_conv_layers(self):\n",
    "        #TODO: create Conv layers from the architecture_config list\n",
    "        layers = []\n",
    "        in_channels = self.in_channel\n",
    "\n",
    "        for x in architecture_config:\n",
    "            if type(x) == tuple:\n",
    "                layers += [\n",
    "                    nn.Conv2d(in_channels=in_channels, out_channels=x[1], kernel_size=x[0], stride=x[2], padding=x[3]),\n",
    "                    nn.BatchNorm2d(x[1]),\n",
    "                    nn.LeakyReLU(0.1)\n",
    "                ]\n",
    "                in_channels = x[1]\n",
    "            elif type(x) == str:\n",
    "                layers += [nn.MaxPool2d(kernel_size=(2, 2), stride=(2, 2))]\n",
    "            elif type(x) == list:\n",
    "                conv1 = x[0]\n",
    "                conv2 = x[1]\n",
    "                num_repeats = x[2]\n",
    "                for _ in range(num_repeats):\n",
    "                    layers += [\n",
    "                        nn.Conv2d(in_channels, conv1[1], kernel_size=conv1[0], stride=conv1[2], padding=conv1[3]),\n",
    "                        nn.BatchNorm2d(conv1[1]),\n",
    "                        nn.LeakyReLU(0.1),\n",
    "                        nn.Conv2d(conv1[1], conv2[1], kernel_size=conv2[0], stride=conv2[2], padding=conv2[3]),\n",
    "                        nn.BatchNorm2d(conv2[1]),\n",
    "                        nn.LeakyReLU(0.1)\n",
    "                    ]\n",
    "                    in_channels = conv2[1]\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def _create_fcs(self, split_size, num_boxes, num_classes):\n",
    "        S, B, C = split_size, num_boxes, num_classes\n",
    "        return nn.Sequential(\n",
    "            nn.Linear(1024 * S * S, 1024),\n",
    "            nn.LeakyReLU(0.1),\n",
    "            nn.Linear(1024, S * S * (C + B * 5)),\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test model correctness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Test model correctness\n",
    "\n",
    "model = YOLONet(split_size=7, num_boxes=2, num_classes=20)\n",
    "input = torch.randn(4, 3, 448, 448)\n",
    "out = model(input)\n",
    "\n",
    "assert out.shape == (4, 7 * 7 * (20 + 2 * 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Define loss function\n",
    "class YoloLoss(nn.Module):\n",
    "    def __init__(self, S = 7, B = 2, C = 20):\n",
    "        super(YoloLoss, self).__init__()\n",
    "        self.mse = nn.MSELoss(reduction = 'sum')\n",
    "        self.S = S\n",
    "        self.B = B\n",
    "        self.C = C\n",
    "        self.lambda_noobj = 0.5\n",
    "        self.lambda_coord = 5\n",
    "\n",
    "    def forward(self, predictions, target):\n",
    "        predictions = predictions.reshape(-1, self.S, self.S, self.C + self.B * 5)\n",
    "\n",
    "        iou_b1 = intersection_over_union(predictions[..., 21:25], target[..., 21:25])\n",
    "        iou_b2 = intersection_over_union(predictions[..., 26:30], target[..., 21:25])\n",
    "        ious = torch.cat([iou_b1.unsqueeze(0), iou_b2.unsqueeze(0)], dim = 0)\n",
    "\n",
    "        iou_maxes, bestbox = torch.max(ious, dim = 0)\n",
    "        exists_box = target[..., 20].unsqueeze(3)\n",
    "\n",
    "        # ======================== #\n",
    "        #   FOR BOX COORDINATES    #\n",
    "        # ======================== #\n",
    "\n",
    "        box_predictions = exists_box * (\n",
    "            (\n",
    "                bestbox * predictions[..., 26:30]\n",
    "                + (1 - bestbox) * predictions[..., 21:25]\n",
    "            )\n",
    "        )\n",
    "\n",
    "        box_targets = exists_box * target[..., 21:25]\n",
    "\n",
    "        box_predictions[..., 2:4] = torch.sign(box_predictions[..., 2:4]) * torch.sqrt(\n",
    "            torch.abs(box_predictions[..., 2:4] + 1e-6)\n",
    "        )\n",
    "\n",
    "        box_targets[..., 2:4] = torch.sqrt(box_targets[..., 2:4])\n",
    "\n",
    "        box_loss = self.mse(\n",
    "            torch.flatten(box_predictions, end_dim = -2),\n",
    "            torch.flatten(box_targets, end_dim = -2),\n",
    "        )\n",
    "\n",
    "        # ==================== #\n",
    "        #   FOR OBJECT LOSS    #\n",
    "        # ==================== #\n",
    "\n",
    "        pred_box = (\n",
    "            bestbox * predictions[..., 25:26] + (1 - bestbox) * predictions[..., 20:21]\n",
    "        )\n",
    "\n",
    "        object_loss = self.mse(\n",
    "            torch.flatten(exists_box * pred_box),\n",
    "            torch.flatten(exists_box * target[..., 20:21]),\n",
    "        )\n",
    "\n",
    "        # ======================= #\n",
    "        #   FOR NO OBJECT LOSS    #\n",
    "        # ======================= #\n",
    "\n",
    "        no_object_loss = self.mse(\n",
    "            torch.flatten((1 - exists_box) * predictions[..., 20:21], start_dim = 1),\n",
    "            torch.flatten((1 - exists_box) * target[..., 20:21], start_dim = 1),\n",
    "        )\n",
    "\n",
    "        no_object_loss += self.mse(\n",
    "            torch.flatten((1 - exists_box) * predictions[..., 25:26], start_dim = 1),\n",
    "            torch.flatten((1 - exists_box) * target[..., 20:21], start_dim = 1),\n",
    "        )\n",
    "\n",
    "        # ================== #\n",
    "        #   FOR CLASS LOSS   #\n",
    "        # ================== #\n",
    "\n",
    "        class_loss = self.mse(\n",
    "            torch.flatten(exists_box * predictions[..., :20], end_dim = -2,),\n",
    "            torch.flatten(exists_box * target[..., :20], end_dim = -2,),\n",
    "        )\n",
    "\n",
    "        loss = (\n",
    "            self.lambda_coord * box_loss  # first two rows in paper\n",
    "            + object_loss  # third row in paper\n",
    "            + self.lambda_noobj * no_object_loss  # forth row\n",
    "            + class_loss  # fifth row\n",
    "        )\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Training script\n",
    "class Trainer:\n",
    "  def __init__(self):\n",
    "    self.lr = 2e-5\n",
    "    self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    self.batch_size = 8\n",
    "    self.weight_decay = 0.0005\n",
    "\n",
    "    self.epoch = 100\n",
    "    self.num_workers = 0\n",
    "    self.pin_memory = True\n",
    "    self.load_model = False\n",
    "\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((448, 448)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.5, 0.5, 0.5],\n",
    "                              std=[0.5, 0.5, 0.5])])\n",
    "    self.train_dataset = VOCDataset(data_dir='./small_data', split='train', transform=transform)\n",
    "    self.trainloader = DataLoader(self.train_dataset, batch_size=self.batch_size, shuffle=True,\n",
    "                                  pin_memory=self.pin_memory, num_workers=self.num_workers, drop_last=False)\n",
    "\n",
    "    self.test_dataset = VOCDataset(data_dir='./small_data', split='test', transform=transform)\n",
    "    self.testloader = DataLoader(self.test_dataset, batch_size=self.batch_size, shuffle=False,\n",
    "                                 pin_memory=self.pin_memory, num_workers=self.num_workers)\n",
    "\n",
    "    self.model = YOLONet(split_size=7, num_boxes=2, num_classes=20).to(self.device)\n",
    "    self.optimizer = optim.Adam(self.model.parameters(), lr=self.lr, weight_decay=self.weight_decay)\n",
    "    self.criterion = YoloLoss()\n",
    "\n",
    "    self.out_dir = './outputs'\n",
    "    os.makedirs(self.out_dir, exist_ok = True)\n",
    "\n",
    "\n",
    "  def run(self):\n",
    "    pbar = tqdm(total = self.epoch, bar_format='{desc}: {percentage:3.0f}% {n_fmt}/{total_fmt} [{elapsed}<{remaining}, {rate_fmt}]', leave=False)\n",
    "    for epoch in range(self.epoch):\n",
    "        if epoch % 10 == 0:\n",
    "          self.test_step()\n",
    "        mean_loss = self.train_step()\n",
    "        pbar.update(1)\n",
    "        pbar.set_description('Epoch: {}, Mean Loss: {:.4f}'.format(epoch, mean_loss))\n",
    "\n",
    "\n",
    "  def train_step(self):\n",
    "    self.model.train()\n",
    "    mean_loss = []\n",
    "    for batch_idx, (image, label) in enumerate(self.trainloader):\n",
    "        image, label = image.to(self.device), label.to(self.device)\n",
    "        output = self.model(image)\n",
    "        loss = self.criterion(output, label)\n",
    "        mean_loss.append(loss.item())\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "    return sum(mean_loss) / len(mean_loss)\n",
    "\n",
    "  @torch.no_grad()\n",
    "  def test_step(self):\n",
    "    self.model.eval()\n",
    "    all_pred_boxes = []\n",
    "    all_true_boxes = []\n",
    "    sample_idx = 0\n",
    "    for batch_idx, (image, label) in enumerate(self.testloader):\n",
    "        image, label = image.to(self.device), label.to(self.device)\n",
    "        batch_size = image.shape[0]\n",
    "        preds = self.model(image).reshape(-1, 7, 7, 30)\n",
    "\n",
    "        true_bboxes = cellboxes_to_boxes(label)\n",
    "        pred_bboxes = cellboxes_to_boxes(preds)\n",
    "\n",
    "        for idx in range(batch_size):\n",
    "            pred_bboxes_nms = non_max_suppression(pred_bboxes[idx])\n",
    "            for box in pred_bboxes_nms:\n",
    "                all_pred_boxes.append([sample_idx] + box.tolist())\n",
    "            for box in true_bboxes[idx]:\n",
    "                if box[1] > 0:\n",
    "                    all_true_boxes.append([sample_idx] + box.tolist())\n",
    "            sample_idx += 1\n",
    "\n",
    "    self.visualize_results(all_true_boxes, all_pred_boxes)\n",
    "    return all_pred_boxes, all_true_boxes\n",
    "\n",
    "  def visualize_results(self, preds, gt):\n",
    "      for idx in range(len(self.test_dataset)):\n",
    "          image, _ = self.test_dataset.__getitem__(idx)\n",
    "          pred_labels = [box for box in preds if box[0] == idx]\n",
    "          gt_labels = [box for box in gt if box[0] == idx]\n",
    "\n",
    "          image = draw_boxes(image, pred_labels, gt_labels)\n",
    "          image.save('{}/{}.png'.format(self.out_dir, idx))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer()\n",
    "trainer.run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

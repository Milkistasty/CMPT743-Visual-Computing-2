{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CMPT743 Lab 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1 (20 mins)\n",
    "\n",
    "Use PyTorch to find the numerical solution to $2x^2-4x+1$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0, x = -0.5330286622047424, equation value = 4.100210189819336\n",
      "step 100, x = 0.9741373062133789, equation value = -0.9985483884811401\n",
      "step 200, x = 0.9995637536048889, equation value = -0.9999996423721313\n",
      "step 300, x = 0.9999926090240479, equation value = -1.0\n",
      "step 400, x = 0.9999992847442627, equation value = -1.0\n",
      "step 500, x = 0.9999992847442627, equation value = -1.0\n",
      "step 600, x = 0.9999992847442627, equation value = -1.0\n",
      "step 700, x = 0.9999992847442627, equation value = -1.0\n",
      "step 800, x = 0.9999992847442627, equation value = -1.0\n",
      "step 900, x = 0.9999992847442627, equation value = -1.0\n",
      "Root approximation: x = 0.9999992847442627\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def equation(x):\n",
    "    return 2*x**2 - 4*x + 1\n",
    "\n",
    "# initialize x with a random value\n",
    "x = torch.randn(1, requires_grad=True)\n",
    "\n",
    "# set up the optimizer, here we use stochastic gradient descent\n",
    "optimizer = torch.optim.SGD([x], lr=0.01)\n",
    "\n",
    "# optimization loop\n",
    "for step in range(1000):\n",
    "    optimizer.zero_grad()\n",
    "    loss = equation(x)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if step % 100 == 0:\n",
    "        print(f\"step {step}, x = {x.item()}, equation value = {loss.item()}\")\n",
    "\n",
    "# result\n",
    "print(f\"Root approximation: x = {x.item()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2 (20 mins)\n",
    "\n",
    "Implement a custom activation function $f(x) = ln(1 + e^x)$, and integrate it into a simple neural network to approximate a simple function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "# Custom Activation Function\n",
    "class CustomActivation(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CustomActivation, self).__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.log(1 + torch.exp(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a Simple Neural Network with the Custom Activation Function\n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(1, 10)  # Input layer\n",
    "        self.fc2 = nn.Linear(10, 10) # Hidden layer\n",
    "        self.fc3 = nn.Linear(10, 1)  # Output layer\n",
    "        self.custom_act = CustomActivation()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.custom_act(self.fc1(x))\n",
    "        x = self.custom_act(self.fc2(x))\n",
    "        return self.fc3(x)\n",
    "\n",
    "# Create the neural network\n",
    "model = SimpleNN()\n",
    "\n",
    "# Define Loss Function and Optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0/1000], Loss: 0.6958268284797668\n",
      "Epoch [100/1000], Loss: 0.44919657707214355\n",
      "Epoch [200/1000], Loss: 0.3510535657405853\n",
      "Epoch [300/1000], Loss: 0.1815216988325119\n",
      "Epoch [400/1000], Loss: 0.09083357453346252\n",
      "Epoch [500/1000], Loss: 0.022584060207009315\n",
      "Epoch [600/1000], Loss: 0.004558686167001724\n",
      "Epoch [700/1000], Loss: 0.002822058042511344\n",
      "Epoch [800/1000], Loss: 0.0020239190198481083\n",
      "Epoch [900/1000], Loss: 0.0015511605888605118\n"
     ]
    }
   ],
   "source": [
    "# Training Data for y = sin(x)\n",
    "x_train = torch.linspace(-10, 10, 1000).view(-1, 1)\n",
    "y_train = torch.sin(x_train)\n",
    "\n",
    "# Training Loop\n",
    "epochs = 1000\n",
    "for epoch in range(epochs):\n",
    "    optimizer.zero_grad()\n",
    "    output = model(x_train)\n",
    "    loss = criterion(output, y_train)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch % 100 == 0:\n",
    "        print(f'Epoch [{epoch}/{epochs}], Loss: {loss.item()}')\n",
    "\n",
    "# Test the model\n",
    "with torch.no_grad():\n",
    "    y_pred = model(x_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_train [tensor([[0.5440],\n",
      "        [0.5271],\n",
      "        [0.5100],\n",
      "        [0.4927],\n",
      "        [0.4752]])], \n",
      "perdiction: tensor([[0.5846],\n",
      "        [0.5623],\n",
      "        [0.5400],\n",
      "        [0.5178],\n",
      "        [0.4956]])\n"
     ]
    }
   ],
   "source": [
    "print(f'y_train [{y_train[0:5]}], \\nperdiction: {y_pred[0:5]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3 (10 mins)\n",
    "\n",
    "Use ONNX/Netron tools to visualize resnet18 architecture from torchvision library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Alienware\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torchvision\\io\\image.py:13: UserWarning: Failed to load image Python extension: 'Could not find module 'C:\\Users\\Alienware\\AppData\\Local\\Programs\\Python\\Python39\\Lib\\site-packages\\torchvision\\image.pyd' (or one of its dependencies). Try using the full path with constructor syntax.'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n",
      "c:\\Users\\Alienware\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Alienware\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to C:\\Users\\Alienware/.cache\\torch\\hub\\checkpoints\\resnet18-f37072fd.pth\n",
      "100%|██████████| 44.7M/44.7M [00:20<00:00, 2.29MB/s]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "\n",
    "# Load pretrained ResNet18 model\n",
    "model = torchvision.models.resnet18(pretrained=True)\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Create a dummy input tensor\n",
    "dummy_input = torch.randn(1, 3, 224, 224)\n",
    "\n",
    "# Export the model\n",
    "torch.onnx.export(model, \n",
    "                  dummy_input, \n",
    "                  \"resnet18.onnx\", \n",
    "                  export_params=True, \n",
    "                  opset_version=10, \n",
    "                  do_constant_folding=True, \n",
    "                  input_names=['input'], \n",
    "                  output_names=['output'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4 (30 mins)\n",
    "\n",
    "Design a neural network in PyTorch that has two parallel input branches, combines these inputs with additional data midway through the network, and then splits into two separate output branches.\n",
    "\n",
    "1. **Input Layer**:\n",
    "    - The network starts with two parallel input branches.\n",
    "    - Each branch should accept an input tensor of shape **`(N, 10)`**, where **`N`** is the batch size.\n",
    "2. **First and Second Branch**:\n",
    "    - **Branch 1 and Branch 2** are identical in structure.\n",
    "    - Each branch consists of the following layers:\n",
    "        - A linear layer that expands the input from 10 to 20 features.\n",
    "        - A ReLU activation layer.\n",
    "        - Another linear layer that further expands from 20 to 30 features.\n",
    "3. **Midway Additional Inputs**:\n",
    "    - After the first and second branches, introduce an additional input tensor of shape **`(N, 5)`**.\n",
    "    - This additional input represents extra features to be combined with the outputs of the two branches.\n",
    "4. **Combination of Branch Outputs and Additional Input**:\n",
    "    - Concatenate the outputs of the two branches (each of shape **`(N, 30)`**) with the additional input (shape **`(N, 5)`**), resulting in a tensor of shape **`(N, 65)`**.\n",
    "5. **Shared Layers After Combination**:\n",
    "    - Pass the combined tensor through a shared linear layer that reduces the dimension from 65 to 50.\n",
    "    - Apply a ReLU activation function.\n",
    "6. **Two Separate Output Branches**:\n",
    "    - Split into two separate output branches after the shared layers.\n",
    "    - **Output Branch 1** and **Output Branch 2**:\n",
    "        - Each output branch consists of a single linear layer that maps the 50 features to a single output feature (shape **`(N, 1)`**)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output 1: tensor([[-0.0299],\n",
      "        [-0.0179],\n",
      "        [-0.0395],\n",
      "        [ 0.0819],\n",
      "        [-0.0777]], grad_fn=<AddmmBackward0>)\n",
      "Output 2: tensor([[ 0.1179],\n",
      "        [ 0.1077],\n",
      "        [ 0.0515],\n",
      "        [-0.0801],\n",
      "        [ 0.2045]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Custom Neural Network\n",
    "class CustomNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CustomNetwork, self).__init__()\n",
    "\n",
    "        # Branch layers (shared between both branches)\n",
    "        self.branch_fc1 = nn.Linear(10, 20)\n",
    "        self.branch_fc2 = nn.Linear(20, 30)\n",
    "\n",
    "        # Shared layers afte r combining the branches\n",
    "        self.shared_fc1 = nn.Linear(65, 50)\n",
    "\n",
    "        # Separate output branches\n",
    "        self.output_branch1 = nn.Linear(50, 1)\n",
    "        self.output_branch2 = nn.Linear(50, 1)\n",
    "\n",
    "    def forward(self, x1, x2, additional_input):\n",
    "        # First and second branches\n",
    "        x1 = F.relu(self.branch_fc1(x1))\n",
    "        x1 = self.branch_fc2(x1)\n",
    "\n",
    "        x2 = F.relu(self.branch_fc1(x2))\n",
    "        x2 = self.branch_fc2(x2)\n",
    "\n",
    "        # Combine the outputs of the two branches with the additional input\n",
    "        combined = torch.cat((x1, x2, additional_input), dim=1)\n",
    "\n",
    "        # Shared layers after combination\n",
    "        combined = F.relu(self.shared_fc1(combined))\n",
    "\n",
    "        # Two separate output branches\n",
    "        output1 = self.output_branch1(combined)\n",
    "        output2 = self.output_branch2(combined)\n",
    "\n",
    "        return output1, output2\n",
    "\n",
    "# Create the neural network\n",
    "model = CustomNetwork()\n",
    "\n",
    "# Example usage\n",
    "N = 5  # Example batch size\n",
    "input1 = torch.randn(N, 10)\n",
    "input2 = torch.randn(N, 10)\n",
    "additional_input = torch.randn(N, 5)\n",
    "output1, output2 = model(input1, input2, additional_input)\n",
    "\n",
    "print(\"Output 1:\", output1)\n",
    "print(\"Output 2:\", output2)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
